{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiez automatiquement des biens de consommation - partie textuelle - modélisation non supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sommaire de tous les notebooks de ce dossier__\n",
    "\n",
    "1. Analyse exploratoire des données textuelles\n",
    "2. Analyse exploratoire des données visuelles__\n",
    "3. __Prétraitement, feature extraction et faisabilité textuelle - premiers modèles__  \n",
    "4. Prétraitement, feature extraction et faisabilité visuelle - premiers modèles\n",
    "\n",
    "Dans ce notebook, nous allons traiter du troisième point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les critères à remplir dans ce notebook\n",
    "   1. __Prétraiter des données textes pour obtenir un jeu de données exploitable.__\n",
    "      - [ ] CE1 Vous avez nettoyé les champs de texte (suppression de la ponctuation et des mots de liaison, mise en minuscules)\n",
    "      - [x] CE2 Vous avez écrit une fonction permettant de “tokeniser” une phrase.\n",
    "      - [x] CE3 Vous avez écrit une fonction permettant de “stemmer” une phrase.\n",
    "      - [x] CE4 Vous avez écrit une fonction permettant de “lemmatiser” une phrase.\n",
    "      - [ ] CE5 Vous avez construit des features (\"feature engineering\") de type bag-of-words (bag-of-words standard : comptage de mots, et Tf-idf), avec des étapes de nettoyage supplémentaires : seuil de fréquence des mots, normalisation des mots.\n",
    "      - [ ] CE6 Vous avez testé une phrase ou un court texte d'exemple, pour illustrer la bonne réalisation des 5 étapes précédentes.\n",
    "      - [ ] CE7 Vous avez, en complément de la démarche de type “bag-of-words”, mis en oeuvre 3 démarches de word/sentence embedding : Word2Vec (ou Doc2Vec ou Glove ou FastText), BERT, et USE (Universal Sentence Encoder).\n",
    "      - [ ] CE8 Vous vous êtes assurés que le texte traité ne relève pas d’une propriété intellectuelle dont l’utilisation ou la modification est interdite.\n",
    "\n",
    "   2. __Mettre en œuvre des techniques de réduction de dimension.__\n",
    "      - [ ] CE1 Vous avez justifié la nécessité de la réduction de dimension. \n",
    "      - [ ] CE2 Vous avez appliqué une méthode de réduction de dimension adaptée à la problématique (ex. : ACP). \n",
    "      - [ ] CE3 Vous avez justifié le choix des valeurs des paramètres dans la méthode de réduction de dimension retenue (ex. : le nombre de dimensions conservées pour l'ACP).  \n",
    " \n",
    "   3. __Représenter graphiquement des données à grandes dimensions.__\n",
    "      - [ ] CE1 Vous avez mis en œuvre au moins une technique de réduction de dimension (via LDA, ACP, T-SNE, UMAP ou autre technique).\n",
    "      - [ ] CE2 Vous avez réalisé au moins un graphique représentant les données réduites en 2D (par exemple affichage des 2 composantes du T-SNE).\n",
    "      - [ ] CE3 Vous avez réalisé et formalisé une analyse du graphique en 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation de l'environnement\n",
    "\n",
    "__Importation des librairies, fonctions et le jeu de données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/Audrey/Documents/github_repositories/DS_Master_project_6/\")\n",
    "\n",
    "#import my_packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importation des librairies et du jeu de données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Audrey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Audrey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Audrey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Audrey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'PATH_output' from 'my_packages' (/Users/Audrey/Documents/github_repositories/DS_Master_project_6/my_packages/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmy_packages\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (PATH_input, PATH_output, RANDOM_STATE, flipkart_stopword, \n\u001b[1;32m      4\u001b[0m  tokenize, stemming, lemmatisation, transform_bow_fct, transform_bow_stem_fct,\n\u001b[1;32m      5\u001b[0m  transform_bow_lemm_fct, transform_dl_fct, cleaning_text,\n\u001b[1;32m      6\u001b[0m  bold_print, freq_threshold, variance_needed_plot, premiere_evaluation_graphique,\n\u001b[1;32m      7\u001b[0m  visu_fct, eval_metrics_df, conf_mat_transform, confusion_matrix_plot,\n\u001b[1;32m      8\u001b[0m  classification_report_df, error_plot)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m word_tokenize\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'PATH_output' from 'my_packages' (/Users/Audrey/Documents/github_repositories/DS_Master_project_6/my_packages/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from my_packages import (PATH_input, PATH_output, RANDOM_STATE, flipkart_stopword, \n",
    " tokenize, stemming, lemmatisation, transform_bow_fct, transform_bow_stem_fct,\n",
    " transform_bow_lemm_fct, transform_dl_fct, cleaning_text,\n",
    " bold_print, freq_threshold, variance_needed_plot, premiere_evaluation_graphique,\n",
    " visu_fct, eval_metrics_df, conf_mat_transform, confusion_matrix_plot,\n",
    " classification_report_df, error_plot)\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/Audrey/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/Audrey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Audrey/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Télécharger la liste de stopwords\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "#from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "#from re import sub\n",
    "#from string import punctuation\n",
    "#import unicodedata\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Mise en place du format de l'affichage des données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pour afficher l'ensemble du texte dans une cellule de dataframe\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Importation du jeu de données textuel__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: '/Users/audreyterrien/Documents/github_repositories/DS_Master_project_6/input/flipkart_com-ecommerce_sample_1050.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Réimportation uniquement des données textuelles nécessaires\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_input\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mflipkart_com-ecommerce_sample_1050.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m    \n\u001b[1;32m      3\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCATEGORY\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mproduct_category_tree\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;130;01m\\\"\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m >>\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m      4\u001b[0m df \u001b[38;5;241m=\u001b[39m df[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCATEGORY\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdescription\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ocds6/lib/python3.9/site-packages/pandas/io/parsers/readers.py:948\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    944\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    945\u001b[0m )\n\u001b[1;32m    946\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 948\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ocds6/lib/python3.9/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    610\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 611\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    614\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ocds6/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1448\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1448\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ocds6/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1705\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1703\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1704\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1705\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1708\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1709\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1710\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1711\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1712\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1715\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1716\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m/usr/local/Caskroom/miniconda/base/envs/ocds6/lib/python3.9/site-packages/pandas/io/common.py:863\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    858\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    859\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    861\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    862\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 863\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    871\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    872\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mPermissionError\u001b[0m: [Errno 13] Permission denied: '/Users/audreyterrien/Documents/github_repositories/DS_Master_project_6/input/flipkart_com-ecommerce_sample_1050.csv'"
     ]
    }
   ],
   "source": [
    "# Réimportation uniquement des données textuelles nécessaires\n",
    "df = pd.read_csv(PATH_input+'flipkart_com-ecommerce_sample_1050.csv')    \n",
    "df['CATEGORY'] = df['product_category_tree'].map(lambda x: x.split(\"[\\\"\")[1].split(\" >>\", 1)[0])\n",
    "df = df[['CATEGORY', 'description']]\n",
    "df.columns = ['CATEGORY', 'TEXT']\n",
    "df[df.index==222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Prétraitement textuels\n",
    "\n",
    "# tokenisation\n",
    "df['TOKEN'] = df['TEXT'].map(tokenize)\n",
    "\n",
    "# stemming\n",
    "df['STEM'] = df['TEXT'].map(stemming)\n",
    "\n",
    "# Lemmatisation\n",
    "df['LEMM'] = df['TEXT'].map(lemmatisation)\n",
    "\n",
    "# tokenisation + transformation en phrases\n",
    "df['sentence_bow'] = df['TEXT'].map(transform_bow_fct)\n",
    "\n",
    "# stemming + transformation en phrases\n",
    "df['sentence_stem'] = df['TEXT'].map(transform_bow_stem_fct)\n",
    "\n",
    "# Lemmatisation + transformation en phrases\n",
    "df['sentence_lemm'] = df['TEXT'].map(transform_bow_lemm_fct)\n",
    "\n",
    "# tokenisation pour modèle DL + transformation en phrases\n",
    "df['sentence_dl'] = df['TEXT'].map(transform_dl_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taille maximum des features selon les techniques de prétraitement du texte\n",
    "df['length_bow'] = df['sentence_bow'].apply(lambda x : len(word_tokenize(x)))\n",
    "df['length_dl'] = df['sentence_dl'].apply(lambda x : len(word_tokenize(x)))\n",
    "df['length_bow_lemm'] = df['sentence_lemm'].apply(lambda x : len(word_tokenize(x)))\n",
    "df['length_bow_stem'] = df['sentence_stem'].apply(lambda x : len(word_tokenize(x)))\n",
    "\n",
    "print(\"Nombre de mots maximum dans le bag-of-words avec la méthode de tokénisation :\", df['length_bow'].max())\n",
    "print(\"Nombre de mots maximum dans le bag-of-words avec la méthode de lemmatisation :\", df['length_bow_lemm'].max())\n",
    "print(\"Nombre de mots maximum dans le bag-of-words avec la méthode de stemming :\", df['length_bow_stem'].max())\n",
    "print(\"Nombre de mots maximum pour le deep learning (DL) avec la méthode de tokénisation :\", df['length_dl'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Premier exemple pour chaque type de modification (CE6)\n",
    "row_index = 220\n",
    "example_df = pd.DataFrame({'0. TEXT': df['TEXT'][row_index], \n",
    "                           '1. CLEAN_TEXT': cleaning_text(df['TEXT'][row_index]),\n",
    "                           '2. TOKENIZATION': str(tokenize(df['TEXT'][row_index])),\n",
    "                           '2B. TOKENIZED DOC': str(transform_bow_fct(df['TEXT'][row_index])),\n",
    "                           '3. STEMMING': str(stemming(df['TEXT'][row_index])),\n",
    "                           '3B. STEMMATIZED DOC': str(transform_bow_stem_fct(df['TEXT'][row_index])),\n",
    "                           '4. LEMMATIZATION': str(lemmatisation(df['TEXT'][row_index])),\n",
    "                           '4B. LEMMATIZED DOC': str(transform_bow_lemm_fct(df['TEXT'][row_index])),\n",
    "                          }, index=['Test on sample 220']).rename_axis('Type de modification').T\n",
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# librairies pour la réduction des dimensions\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "import seaborn as sns\n",
    "\n",
    "# librairie pour l'évaluation du modèle \n",
    "from sklearn.metrics import (homogeneity_score, v_measure_score, completeness_score, \n",
    "                             confusion_matrix, precision_score, recall_score, \n",
    "                             accuracy_score, adjusted_rand_score, classification_report)\n",
    "from joblib import dump, load\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\"\"\"\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# librairies pour l'extraction des features\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize, FunctionTransformer, LabelEncoder\n",
    "\n",
    "# librairies pour la réduction des dimensions\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, PCA\n",
    "\n",
    "# librairies pour le clustering\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, SpectralClustering, Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# librairie pour l'évaluation du modèle \n",
    "from sklearn.metrics import (homogeneity_score, v_measure_score, completeness_score, \n",
    "                             confusion_matrix, precision_score, recall_score, \n",
    "                             accuracy_score, adjusted_rand_score, classification_report)\n",
    "from joblib import dump, load\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cache les warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"sklearn\")\n",
    "#def fonction_qui_peut_generer_un_warning():\n",
    "#    # Exemple de code susceptible de générer un UserWarning\n",
    "#    warnings.warn(\"Ceci est un UserWarning\", UserWarning)\n",
    "\n",
    "# Ignorer tous les UserWarning\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "# Appel de la fonction\n",
    "#fonction_qui_peut_generer_un_warning()\n",
    "\n",
    "# Rétablir le comportement par défaut pour les UserWarning\n",
    "#warnings.filterwarnings(\"default\", category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print en gras le texte\n",
    "def bold_print(texte):\n",
    "    bold_text = f\"\\033[1m{texte}\\033[0m\" \n",
    "    print(bold_text)\n",
    "\n",
    "# Fonction créant un seuil de fréquence de mots (CE5)\n",
    "def freq_threshold(X, threshold):\n",
    "    \"\"\"\n",
    "    Supprime les mots dont la fréquence est inférieure à un certain seuil.\n",
    "    \"\"\"\n",
    "    # Calcul de la fréquence de chaque mot\n",
    "    freq = np.ravel(X.sum(axis=0))\n",
    "    # Indices des mots à garder\n",
    "    idx = np.where(freq >= threshold)[0]\n",
    "    return X[:, idx]\n",
    "  #('threshold', FunctionTransformer(freq_threshold)),\n",
    "\n",
    "# Tracer le graphique de la variance expliquée cumulée en fonction du nombre de composants\n",
    "def variance_needed_plot(cumulative_var, n_components):\n",
    "  plt.plot(cumulative_var)\n",
    "  plt.axvline(x=n_components, color='r', linestyle='--')\n",
    "  plt.axhline(y=0.99, color='g', linestyle='--')\n",
    "  plt.title(\"Accumulation du pourcentage de l'explication la variance \\nen fonction du nombre de composants\", size=12)\n",
    "  plt.xlabel('Nombre de composants', size=11)\n",
    "  plt.text(n_components-40, 0.3, f'n_components={n_components}', rotation=90, size=11, color='r')\n",
    "  plt.text(1, 1, f'99% de la variance expliquée', rotation=0, size=11, color='g')\n",
    "  plt.ylabel('Variance expliquée cumulée', size=11)\n",
    "  plt.show()\n",
    "\n",
    "# Créer le barplot avec seaborn\n",
    "def premiere_evaluation_graphique(pred_label):\n",
    "  # Convertir les prédictions en libellés\n",
    "  df_pred = pd.DataFrame({'pred_label': le.inverse_transform(pred_label)})\n",
    "  \n",
    "  # Groupby sur les prédictions et compter les occurrences\n",
    "  df_count = df_pred.groupby('pred_label').size().reset_index(name='Décompte')\n",
    "  display(df_count)\n",
    "\n",
    "  # Créer le barplot avec seaborn\n",
    "  sns.set_style('whitegrid')\n",
    "  fig, ax = plt.subplots(figsize=(10,6))\n",
    "  sns.barplot(y='pred_label', x='Décompte', data=df_count, palette=palette)\n",
    "  plt.xticks(rotation=45, ha='right', fontsize=12)\n",
    "  plt.xlabel('Prédiction', fontsize=14, fontweight='bold')\n",
    "  plt.ylabel('Nombre d\\'occurrences', fontsize=14, fontweight='bold')\n",
    "  plt.title('Occurrences des prédictions', fontsize=16, fontweight='bold')\n",
    "  plt.show()\n",
    "\n",
    "# Affichage graphique représentant les données réduites en 2D (CE2)\n",
    "def visu_fct(X_reduction_model, true_label, pred_label):\n",
    "    fig = plt.figure(figsize=(15,6))\n",
    "    \n",
    "    ax = fig.add_subplot(121)\n",
    "    scatter = ax.scatter(X_reduction_model[:,0],\n",
    "                         X_reduction_model[:,1], \n",
    "                         c=true_label, \n",
    "                         cmap=my_cmap,\n",
    "                         norm=norm)\n",
    "    ax.legend(handles=scatter.legend_elements()[0], \n",
    "              labels=cat_list, \n",
    "              loc=\"best\", \n",
    "              title=\"Catégorie\")\n",
    "    plt.xlabel('tsne1', fontsize = 15, fontweight = 'bold')\n",
    "    plt.ylabel('tsne2', fontsize = 15, fontweight = 'bold')\n",
    "    plt.title('Représentation des descriptions\\npar catégories réelles',\n",
    "              fontsize=18, \n",
    "              pad=5, \n",
    "              fontweight='bold')\n",
    "    \n",
    "    ax = fig.add_subplot(122)\n",
    "    scatter = ax.scatter(X_reduction_model[:,0],\n",
    "                         X_reduction_model[:,1], \n",
    "                         c=pred_label, \n",
    "                         cmap=my_cmap,\n",
    "                         norm=norm)\n",
    "    ax.legend(handles=scatter.legend_elements()[0], \n",
    "              labels=set(pred_label), \n",
    "              loc=\"best\", \n",
    "              title=\"Clusters\")\n",
    "    plt.xlabel('tsne1', fontsize = 15, fontweight = 'bold')\n",
    "    plt.ylabel('tsne2', fontsize = 15, fontweight = 'bold')\n",
    "    plt.title('Représentation des descriptions\\npar clusters', \n",
    "              fontsize=18, \n",
    "              pad=5, \n",
    "              fontweight='bold')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# fonction d'évaluation d'un modèle présentés sous forme d'un tableau\n",
    "def eval_metrics_df(true_label, pred_label):\n",
    "    v_measure = np.round(v_measure_score(true_label, pred_label),4)\n",
    "    completeness = np.round(completeness_score(true_label, pred_label),4)\n",
    "    homogeneity = np.round(homogeneity_score(true_label, pred_label),4)\n",
    "    ARI = np.round(adjusted_rand_score(true_label, pred_label),4)\n",
    "    metric_df = pd.DataFrame([[v_measure, completeness, homogeneity, ARI]], \n",
    "                             columns=['v_measure', \n",
    "                                      'completeness', \n",
    "                                      'homogeneity', \n",
    "                                      'ARI'])\n",
    "    return metric_df\n",
    "\n",
    "# Modifcation des labels prédits pour les rapprocher le plus possible des vrais labels\n",
    "def conf_mat_transform(y_true, y_pred) :\n",
    "    conf_mat = confusion_matrix(y_true,y_pred)\n",
    "    corresp = np.argmax(conf_mat, axis=0)\n",
    "    print (\"Correspondance des clusters : \", corresp)\n",
    "    labels = pd.Series(y_true, name=\"y_true\").to_frame()\n",
    "    labels['y_pred'] = y_pred\n",
    "    labels['y_pred_transform'] = labels['y_pred'].apply(lambda x : corresp[x]) \n",
    "    return labels['y_pred_transform']\n",
    "\n",
    "\n",
    "# Visualiser de la matrice de confusion sous forme graphique\n",
    "def confusion_matrix_plot(true_label, pred_label):\n",
    "  conf_mat = confusion_matrix(true_label, pred_label)\n",
    "  df_cm = pd.DataFrame(conf_mat, index=[label for label in cat_list],\n",
    "                      columns=[i for i in \"0123456\"])\n",
    "\n",
    "  #sns.set(style=\"white\")\n",
    "  plt.subplots(figsize=(9, 6))\n",
    "  sns.heatmap(df_cm, annot=True, cmap='YlOrBr', fmt='d', cbar=False, \n",
    "              square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n",
    "  plt.title('Matrice de confusion entre les clusters créés et les catégories des produits réelles')\n",
    "  plt.xlabel('Clusters')\n",
    "  plt.ylabel('Catégories')\n",
    "  plt.xticks(rotation=45)\n",
    "  plt.yticks(rotation=0)\n",
    "  plt.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "# Calculer et créer le rapport de classification\n",
    "def classification_report_df(true_label, pred_label):\n",
    "  report = classification_report(true_label,pred_label, output_dict=True)\n",
    "  return pd.DataFrame(report).transpose().round(2)\n",
    "\n",
    "def error_plot(true_label, pred_label):\n",
    "  # Créer un dataframe pour les prédictions\n",
    "  df_pred = pd.DataFrame({'true_label': le.inverse_transform(true_label), 'pred_label': le.inverse_transform(pred_label)})\n",
    "  df_errors = (\n",
    "    df_pred\n",
    "    .groupby(['true_label', 'pred_label'])\n",
    "    .size()\n",
    "    .reset_index(name='Décompte')\n",
    "    [['true_label', 'pred_label', 'Décompte']])\n",
    "  display(df_errors)\n",
    "  \n",
    "  # Création du graphique à barres\n",
    "  sns.barplot(data=df_errors, y='true_label', x='Décompte', hue='pred_label', hue_order=cat_list, palette=palette)\n",
    "  # Positionner la boîte de la légende en dehors du graphe\n",
    "  plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., title='Catégorie prédite')\n",
    "  # Affichage du graphe\n",
    "  plt.xlabel(\"Nombre de produits\")\n",
    "  plt.ylabel(\"Catégorie réelle\")\n",
    "  plt.title(\"Visualisation sur erreurs de catégorisation:\\nNombre de produits prédits en fonction de leur vrai catégorie\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Télécharger la liste de mots vides pour la langue anglaise\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Importer la liste de mots vides pour la langue anglaise\n",
    "stop_word = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "# Votre liste personnalisée de mots vides\n",
    "flipkart_stopword = {'word1', 'word2', 'word3'}  # Ajoutez vos propres mots vides ici\n",
    "\n",
    "# Assurez-vous que tous les mots de flipkart_stopword sont également dans stop_words\n",
    "custom_stop_words = flipkart_stopword.intersection(stop_word)\n",
    "custom_stop_words = list(flipkart_stopword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.pipeline import Pipeline\n",
    "# librairies pour la réduction des dimensions\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
    "#from sklearn.base import BaseEstimator, TransformerMixin\n",
    "#from sklearn.cluster import KMeans\n",
    "#import seaborn as sns\n",
    "\n",
    "# librairie pour l'évaluation du modèle \n",
    "#from sklearn.metrics import (homogeneity_score, v_measure_score, completeness_score, \n",
    "#                             confusion_matrix, precision_score, recall_score, \n",
    "#                             accuracy_score, adjusted_rand_score, classification_report)\n",
    "#from joblib import dump, load\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#from matplotlib.colors import ListedColormap, BoundaryNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, make_scorer\n",
    "\n",
    "# Définir la fonction de scoring personnalisée\n",
    "def silhouette_scorer(estimator, X):\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    # La silhouette score nécessite au moins 2 labels pour être calculée.\n",
    "    if len(set(cluster_labels)) > 1:\n",
    "        score = silhouette_score(X, cluster_labels)\n",
    "        return score\n",
    "    else:\n",
    "        return -1  # retourner une valeur négative si un seul cluster est formé\n",
    "\n",
    "# Utiliser make_scorer pour créer un objet scorer utilisable par GridSearchCV\n",
    "custom_scorer = make_scorer(silhouette_scorer, greater_is_better=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éléments à fixer pour ce modèle\n",
    "feat_ = 'sentence_lemm'\n",
    "model_name = 'tf_lem_tsne_01'\n",
    "\n",
    "# Technique de preprocessing de langage choisi\n",
    "preprocessing_ = TfidfVectorizer(stop_words=flipkart_stopword,\n",
    "                                ngram_range=(1,2), #combine de mot unigrammes (un mot) et de bigramme (mot composé de 2 mots)\n",
    "                                min_df=2, #min de deux mots\n",
    "                                max_df=0.98,\n",
    "                                #vocabulary=custom_vocab\n",
    "                                )\n",
    "\n",
    "\n",
    "# Création d'un premier pipeline permettant de calculer le nombre de dimension \n",
    "# à garder pour conserver une explication de la variance à une hauteur de 99%\n",
    "# pour le vrai pipeline et ainsi diminuer le temps d'éxecution de la pipeline\n",
    "pipeline_ = Pipeline([\n",
    "    ('preprocess', preprocessing_),\n",
    "    ('reducDimension', TruncatedSVD(n_components=4000)),\n",
    "])\n",
    "\n",
    "# Entraîner le pipeline\n",
    "display(pipeline_.fit(df[feat_]))\n",
    "\n",
    "# Affichage du vocabulaire\n",
    "vocab = preprocessing_.vocabulary_\n",
    "print(vocab)\n",
    "\n",
    "# Calculer la variance expliquée cumulée\n",
    "explained_variance = pipeline_.named_steps['reducDimension'].explained_variance_ratio_\n",
    "cumulative_var = np.cumsum(explained_variance)\n",
    "\n",
    "# Trouver le nombre de dimensions nécessaires pour atteindre 0.99 de la variance expliquée\n",
    "n_components = np.argmax(cumulative_var >= 0.99) + 1\n",
    "\n",
    "# Représentation graphique du nombre de dimensions minimums pour pouvoir \n",
    "# conserver 99% de l'explication de la variance observée\n",
    "variance_needed_plot(cumulative_var=cumulative_var, n_components=n_components)\n",
    "\n",
    "# Rajout du nombre de composants nécéssaire pour garder 99% de la variance +\n",
    "# Détermination des hyperparamètres liées à la T-SNE\n",
    "pipeline_ = Pipeline([\n",
    "    # Preprocessing: construction des features\n",
    "    ('preprocess', preprocessing_),\n",
    "    # Réduction des dimensions pour ne garder que 99% de l'explication de la variance\n",
    "    ('reducDimension', TruncatedSVD(n_components=n_components)),\n",
    "    # Réduction TSNE \n",
    "    ('tsne', TSNE(n_components=2, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# Entraîner le pipeline\n",
    "display(pipeline_.fit(df[feat_]))\n",
    "\n",
    "# Observation sur les paramètres par défaut choisi lors de son entraînement\n",
    "# pour la TSNE\n",
    "print('La perplexité du TSNE a été fixé automatiquement par le modèle a', pipeline_.get_params()['tsne__perplexity'])\n",
    "\n",
    "\n",
    "# Définir les paramètres à tester\n",
    "params_ = {\n",
    "    # Paramètres pour TF-IDF\n",
    "    'preprocess__ngram_range': [(1, 1), (1, 2)],  # Teste les unigrammes et bigrammes\n",
    "    'preprocess__min_df': sp_randint(1, 3),  # Teste min_df à 1, 2, 3\n",
    "    #'preprocess__max_df': uniform(0.75, 0.95),  # Teste max_df entre 0.75 et 0.95\n",
    "\n",
    "    # Paramètres pour t-SNE\n",
    "    'tsne__perplexity': sp_randint(10, 15),  # Teste perplexity entre 10 et 80\n",
    "    #'tsne__n_iter': sp_randint(500, 5000),  # Teste n_iter entre 500 et 5000\n",
    "}\n",
    "\n",
    "# Créer un objet RandomizedSearchCV pour trouver les meilleurs paramètres\n",
    "random_search_ = RandomizedSearchCV(estimator=pipeline_, \n",
    "                                        param_distributions=params_, \n",
    "                                        cv=2, \n",
    "                                        n_iter=10,\n",
    "                                        n_jobs=-1, \n",
    "                                        verbose=1,\n",
    "                                        random_state=42, \n",
    "                                        scoring=silhouette_scorer\n",
    "                                        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search_.fit(df[feat_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, make_scorer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Configuration du pipeline\n",
    "pipeline_ = Pipeline([\n",
    "    ('preprocess', TfidfVectorizer(stop_words='english')),\n",
    "    ('reducDimension', TruncatedSVD(n_components=50)),  # Réduction de dimension initiale pour optimiser t-SNE\n",
    "    ('tsne', TSNE(n_components=2, random_state=42)),   # t-SNE pour la visualisation en 2D\n",
    "])\n",
    "\n",
    "# Définition de la fonction de scoring adaptée\n",
    "def silhouette_scorer(estimator, X):\n",
    "    # Appliquer fit_predict pour obtenir les labels des clusters\n",
    "    cluster_labels = estimator.fit_predict(X)\n",
    "    return silhouette_score(X, cluster_labels)\n",
    "\n",
    "# Création d'un scorer utilisable par GridSearchCV ou RandomizedSearchCV\n",
    "silhouette_custom_scorer = make_scorer(silhouette_scorer)\n",
    "\n",
    "# Utilisation dans RandomizedSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "param_distributions = {\n",
    "    #'cluster__n_clusters': sp_randint(3, 10),\n",
    "    'tsne__perplexity': sp_randint(10, 20)\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline_, \n",
    "                            param_distributions=param_distributions, \n",
    "                            n_iter=10, \n",
    "                            scoring=silhouette_custom_scorer, verbose=1, cv=2)\n",
    "search.fit(df[feat_])  # Supposons que df[feat_] est votre jeu de données textuelles\n",
    "\n",
    "print(\"Meilleurs paramètres trouvés :\", search.best_params_)\n",
    "print(\"Meilleur score de silhouette :\", search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Afficher les meilleurs paramètres trouvés\n",
    "print(\"Les Meilleurs paramètres pour l'étape TSNE :\")\n",
    "print(random_search_.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observation sur les paramètres par défaut choisi lors de son entraînement\n",
    "# pour la TSNE\n",
    "print('La perplexité du TSNE a été fixé automatiquement par le modèle a', pipeline_.get_params()['tsne__perplexity'])\n",
    "\n",
    "##%%time\n",
    "# Définir les paramètres à tester\n",
    "tsne_params = {\n",
    "    'tsne__perplexity': [5, 10]#, 20, 30]\n",
    "    #'tsne__n_iter': sp_randint(500, 5000),\n",
    "}\n",
    "\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Fonction pour calculer la silhouette score pour TSNE\n",
    "def silhouette_scorer(estimator, X):\n",
    "    transformed = estimator.fit_transform(X)\n",
    "    if hasattr(estimator, 'labels_'):\n",
    "        labels = estimator.labels_\n",
    "    else:\n",
    "        labels = estimator.predict(X)\n",
    "    return silhouette_score(X, labels)\n",
    "\n",
    "# Créer un objet RandomizedSearchCV pour trouver les meilleurs paramètres\n",
    "tsne_random_search = GridSearchCV(estimator=pipeline_, \n",
    "                                        param_grid=tsne_params, \n",
    "                                        cv=3, \n",
    "                                        #n_iter=20,\n",
    "                                        n_jobs=-1, \n",
    "                                        verbose=1,\n",
    "                                        #random_state=RANDOM_STATE, \n",
    "                                        scoring=silhouette_scorer,\n",
    "                                        )\n",
    "tsne_random_search.fit(df[feat_])\n",
    "\n",
    "# Pour afficher les meilleurs paramètres trouvés et les utiliser pour la configuration de TSNE\n",
    "#print(\"Les Meilleurs paramètres pour l'étape TSNE :\")\n",
    "#print(tsne_random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Afficher les meilleurs paramètres trouvés\n",
    "print(\"Les Meilleurs paramètres pour l'étape TSNE :\")\n",
    "print(tsne_random_search.best_params_)\n",
    "\n",
    "bold_print(\"Création des 7 clusters et prédiction des clusters\")\n",
    "# Transformation de TSNE pour être compatible avec le reste du pipeline \n",
    "# ce dernier ne supportant pas la fonction predict\n",
    "class TSNETransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=2, perplexity=10, n_iter=2000, random_state=None):\n",
    "        self.n_components = n_components\n",
    "        self.perplexity = perplexity\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "        self.tsne = TSNE(n_components=n_components, perplexity=perplexity, n_iter=n_iter, random_state=random_state)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        return self.tsne.fit_transform(X)\n",
    "\n",
    "# Définir la fonction de transformation qui convertit la sortie de TSNE en un tableau de caractéristiques\n",
    "def tsne_features(tsne_output):\n",
    "    return tsne_output[:, :2]\n",
    "\n",
    "# Créer le pipeline\n",
    "pipeline = Pipeline([\n",
    "    # Preprocessing: construction des features\n",
    "    ('preprocess', preprocessing),\n",
    "    # Réduction des dimensions pour ne garder que 99% de l'explication de la variance\n",
    "    ('reducDimension', TruncatedSVD(n_components=n_components)),\n",
    "    # Réduction TSNE \n",
    "    ('tsne', TSNETransformer(n_components=2, \n",
    "                             perplexity=tsne_random_search.best_params_['tsne__perplexity'], \n",
    "                             n_iter=2000, \n",
    "                             random_state=random_state)),\n",
    "    # Transformation pour être compatible avec le reste du pipeline\n",
    "    ('tsne_features', FunctionTransformer(tsne_features)),\n",
    "    # Création des 7 clusters\n",
    "    ('cluster', KMeans(n_clusters=n_clusters, #-- init: il faut faire varier cette variable et non la perplexité\n",
    "                       # distance - euclidienne alors qu'en réalité il y a aussi manhattan, cosinsimilarity \n",
    "                       random_state=random_state)),\n",
    "\n",
    "])\n",
    "\n",
    "#%%time\n",
    "# Entraîner le pipeline\n",
    "pipeline.fit(df[feat])\n",
    "\n",
    "# Transformation du pipeline\n",
    "X_embedded = pipeline.transform(df[feat])\n",
    "\n",
    "# Création de features à partir du pipeline\n",
    "preprocess = pipeline.named_steps['preprocess']\n",
    "features = preprocess.fit_transform(df[feat])\n",
    "[features.toarray().ravel()]\n",
    "\n",
    "#%%time\n",
    "# Prédiction des labels des clusters\n",
    "pred_label = pipeline.predict(df[feat])\n",
    "\n",
    "# Transformation des pred_label pour les faire matcher avec les true_label \n",
    "pred_label = conf_mat_transform(true_label, pred_label)\n",
    "\n",
    "bold_print(\"Evaluation de la qualité du clustering\")\n",
    "# Première évaluation - visualisation sur les catégories prédits \n",
    "premiere_evaluation_graphique(pred_label)\n",
    "\n",
    "# Représentation graphique des données (CE1)\n",
    "visu_fct(X_embedded, true_label, pred_label)\n",
    "\n",
    "# Création d'une ligne de dataframe contenant les quatre métriques d'évaluation \n",
    "# que sont V-measure, la complétude, homogénéité et l'ARI\n",
    "results = eval_metrics_df(true_label, pred_label)\n",
    "\n",
    "# Enregistrement des résultats dans un tableau\n",
    "results.to_csv(path_data, index=False)\n",
    "\n",
    "# Evaluation des résultats\n",
    "results\n",
    "\n",
    "bold_print(\"Evaluation plus poussée si l'ARI est correct\")\n",
    "# Observation sur la différence entre la réelle catégorie et sa prédiction\n",
    "error_plot(true_label, pred_label)\n",
    "\n",
    "# Visualiser de la matrice de confusion\n",
    "confusion_matrix_plot(true_label, pred_label)\n",
    "\n",
    "# Affichage des métriques d'évaluation precision, recall, f1-score, support\n",
    "classification_report_df(true_label, pred_label)\n",
    "\n",
    "# Enregistrement du pipeline\n",
    "dump(pipeline, path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_report_df(true_label, pred_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown\n",
    "\n",
    "def print_markdown(markdown_text):\n",
    "    markdown = Markdown(markdown_text)\n",
    "    print(markdown.data)\n",
    "\n",
    "# Utilisation de la fonction pour afficher le Markdown généré par classification_report_df\n",
    "report = classification_report_df(true_label, pred_label)\n",
    "print_markdown(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# éléments à fixer pour ce modèle\n",
    "feat = 'sentence_stem'\n",
    "model_name = 'tf_ste_tsne_01'\n",
    "\n",
    "# Télécharger la liste de mots vides pour la langue anglaise\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Importer la liste de mots vides pour la langue anglaise\n",
    "#stop_word = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "# Technique de preprocessing de langage choisi\n",
    "preprocessing = TfidfVectorizer(stop_words=stop_word,\n",
    "                                ngram_range=(1,2), #combine de mot unigrammes (un mot) et de bigramme (mot composé de 2 mots)\n",
    "                                min_df=2, #min de deux mots\n",
    "                                max_df=0.98,\n",
    "                                #vocabulary=custom_vocab\n",
    "                                )\n",
    "\n",
    "# Création d'un premier pipeline permettant de calculer le nombre de dimension \n",
    "# à garder pour conserver une explication de la variance à une hauteur de 99%\n",
    "# pour le vrai pipeline et ainsi diminuer le temps d'éxecution de la pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocess', preprocessing),\n",
    "    ('reducDimension', TruncatedSVD(n_components=4000)),\n",
    "])\n",
    "\n",
    "# Entraîner le pipeline\n",
    "pipeline.fit(df[feat])\n",
    "\n",
    "# Affichage du vocabulaire\n",
    "vocab = preprocessing.vocabulary_\n",
    "print(vocab)\n",
    "\n",
    "# Calculer la variance expliquée cumulée\n",
    "explained_variance = pipeline.named_steps['reducDimension'].explained_variance_ratio_\n",
    "cumulative_var = np.cumsum(explained_variance)\n",
    "\n",
    "# Trouver le nombre de dimensions nécessaires pour atteindre 0.99 de la variance expliquée\n",
    "n_components = np.argmax(cumulative_var >= 0.99) + 1\n",
    "\n",
    "# Représentation graphique du nombre de dimensions minimums pour pouvoir \n",
    "# conserver 99% de l'explication de la variance observée\n",
    "variance_needed_plot(cumulative_var=cumulative_var, n_components=n_components)\n",
    "\n",
    "# Rajout du nombre de composants nécéssaire pour garder 99% de la variance +\n",
    "# Détermination des hyperparamètres liées à la T-SNE\n",
    "pipeline = Pipeline([\n",
    "    # Preprocessing: construction des features\n",
    "    ('preprocess', preprocessing),\n",
    "    # Réduction des dimensions pour ne garder que 99% de l'explication de la variance\n",
    "    ('reducDimension', TruncatedSVD(n_components=n_components)),\n",
    "    # Réduction TSNE \n",
    "    ('tsne', TSNE(n_components=2, random_state=random_state))\n",
    "])\n",
    "\n",
    "# Entraîner le pipeline\n",
    "pipeline.fit(df[feat])\n",
    "\n",
    "# Observation sur les paramètres par défaut choisi lors de son entraînement\n",
    "# pour la TSNE\n",
    "print('La perplexité du TSNE a été fixé automatiquement par le modèle a', pipeline.get_params()['tsne__perplexity'])\n",
    "\n",
    "# Définir les paramètres à tester\n",
    "tsne_params = {\n",
    "    'tsne__perplexity': [10, 20, 30, 40, 50, 60, 70, 80],\n",
    "    #'tsne__n_iter': sp_randint(500, 5000),\n",
    "}\n",
    "\n",
    "# Créer un objet RandomizedSearchCV pour trouver les meilleurs paramètres\n",
    "tsne_random_search = RandomizedSearchCV(estimator=pipeline, \n",
    "                                        param_distributions=tsne_params, \n",
    "                                        cv=5, \n",
    "                                        n_iter=20,\n",
    "                                        n_jobs=-1, \n",
    "                                        verbose=1,\n",
    "                                        random_state=42, \n",
    "                                        scoring='accuracy'\n",
    "                                        )\n",
    "\"\"\"\n",
    "tsne_random_search.fit(df[feat])\n",
    "\n",
    "# Afficher les meilleurs paramètres trouvés\n",
    "print(\"Les Meilleurs paramètres pour l'étape TSNE :\")\n",
    "print(tsne_random_search.best_params_)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocds6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
