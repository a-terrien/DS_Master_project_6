{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifiez automatiquement des biens de consommation - partie textuelle - modélisation non supervisée"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sommaire de tous les notebooks de ce dossier__\n",
    "\n",
    "1. Analyse exploratoire des données textuelles\n",
    "2. Analyse exploratoire des données visuelles__\n",
    "3. __Prétraitement, feature extraction et faisabilité textuelle - premiers modèles__  \n",
    "4. Prétraitement, feature extraction et faisabilité visuelle - premiers modèles\n",
    "\n",
    "Dans ce notebook, nous allons traiter du troisième point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Les critères à remplir dans ce notebook\n",
    "   1. __Prétraiter des données textes pour obtenir un jeu de données exploitable.__\n",
    "      - [ ] CE1 Vous avez nettoyé les champs de texte (suppression de la ponctuation et des mots de liaison, mise en minuscules)\n",
    "      - [x] CE2 Vous avez écrit une fonction permettant de “tokeniser” une phrase.\n",
    "      - [x] CE3 Vous avez écrit une fonction permettant de “stemmer” une phrase.\n",
    "      - [x] CE4 Vous avez écrit une fonction permettant de “lemmatiser” une phrase.\n",
    "      - [ ] CE5 Vous avez construit des features (\"feature engineering\") de type bag-of-words (bag-of-words standard : comptage de mots, et Tf-idf), avec des étapes de nettoyage supplémentaires : seuil de fréquence des mots, normalisation des mots.\n",
    "      - [ ] CE6 Vous avez testé une phrase ou un court texte d'exemple, pour illustrer la bonne réalisation des 5 étapes précédentes.\n",
    "      - [ ] CE7 Vous avez, en complément de la démarche de type “bag-of-words”, mis en oeuvre 3 démarches de word/sentence embedding : Word2Vec (ou Doc2Vec ou Glove ou FastText), BERT, et USE (Universal Sentence Encoder).\n",
    "      - [ ] CE8 Vous vous êtes assurés que le texte traité ne relève pas d’une propriété intellectuelle dont l’utilisation ou la modification est interdite.\n",
    "\n",
    "   2. __Mettre en œuvre des techniques de réduction de dimension.__\n",
    "      - [ ] CE1 Vous avez justifié la nécessité de la réduction de dimension. \n",
    "      - [ ] CE2 Vous avez appliqué une méthode de réduction de dimension adaptée à la problématique (ex. : ACP). \n",
    "      - [ ] CE3 Vous avez justifié le choix des valeurs des paramètres dans la méthode de réduction de dimension retenue (ex. : le nombre de dimensions conservées pour l'ACP).  \n",
    " \n",
    "   3. __Représenter graphiquement des données à grandes dimensions.__\n",
    "      - [ ] CE1 Vous avez mis en œuvre au moins une technique de réduction de dimension (via LDA, ACP, T-SNE, UMAP ou autre technique).\n",
    "      - [ ] CE2 Vous avez réalisé au moins un graphique représentant les données réduites en 2D (par exemple affichage des 2 composantes du T-SNE).\n",
    "      - [ ] CE3 Vous avez réalisé et formalisé une analyse du graphique en 2D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation de l'environnement\n",
    "\n",
    "__Importation des librairies et du jeu de données__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation des librairies utiles pour la partie textuelle\n",
    "#!pip3 install --quiet \"tensorflow-text==2.8.*\" # Mettre le bon chiffre final\n",
    "#!pip3 install --quiet \"tensorflow-text\n",
    "#!pip install umap-learn\n",
    "#!pip install tensorflow==2.8.4\n",
    "#!pip install nltk\n",
    "#!pip install enchant #pour les fautes d'orthographe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importation de librairies de base\n",
    "from google.colab import drive\n",
    "import sys\n",
    "import logging\n",
    "import os\n",
    "from os import listdir\n",
    "os.environ[\"TF_KERAS\"]='1'\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import display\n",
    "\n",
    "# Librairie pour la visualisation \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patheffects as path_effects\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "import plotly.express as px\n",
    "\n",
    "# Nettoyage textuelle\n",
    "import string\n",
    "import re\n",
    "import unicodedata\n",
    "import random\n",
    "from scipy.stats import randint as sp_randint\n",
    "#import enchant\n",
    "\n",
    "# Tokenisation, Stemmatisation et Lemmatisation \n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "#nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Input, GlobalAveragePooling1D, Dropout, Embedding, Dense\n",
    "from keras.models import Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.regularizers import L2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import multiprocessing\n",
    "#import torch\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "# librairies pour l'extraction des features\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import normalize, FunctionTransformer, LabelEncoder\n",
    "\n",
    "# librairies pour la réduction des dimensions\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import TruncatedSVD, LatentDirichletAllocation, PCA\n",
    "\n",
    "# librairies pour le clustering\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering, SpectralClustering, Birch\n",
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# librairie pour l'évaluation du modèle \n",
    "from sklearn.metrics import (homogeneity_score, v_measure_score, completeness_score, \n",
    "                             confusion_matrix, precision_score, recall_score, \n",
    "                             accuracy_score, adjusted_rand_score, classification_report)\n",
    "from joblib import dump, load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation du random seed à 42\n",
    "#np.random.seed(42)\n",
    "#from sklearn.utils import check_random_state\n",
    "#random_state = check_random_state(42)\n",
    "#random_seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terrien_Audrey_1_exploration_textuelle.ipynb\n",
      "Terrien_Audrey_2_exploration_visuelle.ipynb\n",
      "Terrien_Audrey_3_classification_non_supervisée_textuelle.ipynb\n",
      "Terrien_Audrey_4_classification_non_supervisée_ancienne_visuelle.ipynb\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/audreyterrien/Documents/github_repositories/DS_Master_project_6\n"
     ]
    }
   ],
   "source": [
    "cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from my_packages import PATH_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>Home Decor &amp; Festive Needs</td>\n",
       "      <td>Purpledip Kattle Showpiece  -  15 cm (Steel, M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       CATEGORY  \\\n",
       "222  Home Decor & Festive Needs   \n",
       "\n",
       "                                                  TEXT  \n",
       "222  Purpledip Kattle Showpiece  -  15 cm (Steel, M...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Réimportation uniquement des données textuelles nécessaires\n",
    "df = pd.read_csv(PATH_input+'flipkart_com-ecommerce_sample_1050.csv')    \n",
    "df['CATEGORY'] = df['product_category_tree'].map(lambda x: x.split(\"[\\\"\")[1].split(\" >>\", 1)[0])\n",
    "df = df[['CATEGORY', 'description']]\n",
    "df.columns = ['CATEGORY', 'TEXT']\n",
    "df[df.index==222]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Télécharger la liste de stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk import word_tokenize\n",
    "from re import sub\n",
    "from string import punctuation\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les mots à supprimer\n",
    "unwanted_words = set(['', 'products', 'product', 'free', 'rs', 'buy', 'delivery', 'shipping', 'cash', 'cm', \n",
    "                      'flipkart', 'com', 'flipkartcom', 'online', 'price', 'sales', 'features', 'Genuine', 'india',\n",
    "                      'specifications', 'discounts', 'prices', 'key', 'great'])\n",
    "flipkart_stopword = list(stop_words)+list(unwanted_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonctions de nettoyage et de normalisation du texte // version NLTK \n",
    "\n",
    "# Fonctions de nettoyage (CE1)\n",
    "def cleaning_text(doc, stopword_step=True):\n",
    "    # Définir une expression régulière pour identifier les points collés aux \n",
    "    # mots qui ne sont pas des domaines informatiques (.com, .gov, etc.) \n",
    "    # et qui sont dû à des fautes de frappes\n",
    "    pattern_dot = r\"(?<=\\w)\\.(?=\\w)\"\n",
    "\n",
    "    # Définir une expression régulière pour identifier les 's collés aux mots\n",
    "    pattern_s = r\"(?<=\\w)'s\"\n",
    "\n",
    "    # Appliquer les substitutions sur le texte\n",
    "    doc = sub(pattern_dot, \" \", doc)\n",
    "    doc = sub(pattern_s, \"\", doc)\n",
    "\n",
    "    # Passage en minuscule\n",
    "    doc = doc.lower()\n",
    "\n",
    "    # Suppression des chiffres\n",
    "    doc = ''.join([ch for ch in doc if not ch.isdigit()])\n",
    "    \n",
    "    # Suppression de la ponctuation\n",
    "    doc = doc.translate(str.maketrans('', '', punctuation))\n",
    "    \n",
    "    # Suppression des accents\n",
    "    doc = unicodedata.normalize('NFKD', doc).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    \n",
    "    # Choix de suppression des mots communs ou de liaisons \n",
    "    if stopword_step == True:\n",
    "      # Suppression des mots de liaison\n",
    "      #stop_words = set(stopwords.words('english'))\n",
    "      words = doc.split()\n",
    "      words = [word for word in words if word not in flipkart_stopword]\n",
    "      doc = ' '.join(words)\n",
    "\n",
    "    # Correction de fautes d'orthographe hypersimplifié utilisant la librairie\n",
    "    # enchant\n",
    "    # doc = correct_doc(doc) -> pas assez de connaissance \n",
    "    # pour être sûr de l'utiliser correctement\n",
    "\n",
    "    # Suppression des espaces et des retours à la ligne\n",
    "    doc = sub(r'\\s+', ' ', doc).strip()\n",
    "    return doc\n",
    "\n",
    "# fonction qui nettoie et tokénise un texte (CE2)\n",
    "def tokenize(doc):\n",
    "    doc = cleaning_text(doc, stopword_step=True)\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = [token for token in tokens if len(token) > 2]\n",
    "    return tokens\n",
    "\n",
    "# fonction qui nettoie et tokénise pour un texte traité par une technique dl (CE2)\n",
    "def tokenize_dl(doc):\n",
    "    doc = cleaning_text(doc, stopword_step=False)\n",
    "    tokens = word_tokenize(doc)\n",
    "    return tokens\n",
    "\n",
    "# Fonction de stemmatisation (CE3) \n",
    "stemmer = PorterStemmer()\n",
    "def stemming(doc):\n",
    "    tokens = tokenize(doc)\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    return stems\n",
    "\n",
    "# Fonction de lemmatisation (CE4)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatisation(doc):\n",
    "    tokens = tokenize(doc)\n",
    "    lemmas = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Fonction de préparation d'un doc passant par une tokenisation\n",
    "def transform_bow_fct(doc) :\n",
    "    tokens = tokenize(doc)  \n",
    "    doc = ' '.join(tokens)\n",
    "    return doc\n",
    "\n",
    "# Fonction de préparation du texte pour le Deep learning (USE et BERT)\n",
    "def transform_dl_fct(doc) :\n",
    "    tokens = tokenize_dl(doc)  \n",
    "    doc = ' '.join(tokens)\n",
    "    return doc\n",
    "\n",
    "# Fonction de préparation d'un doc passant par une lemmatisation\n",
    "def transform_bow_lemm_fct(doc) :\n",
    "    lemms = lemmatisation(doc)  \n",
    "    doc = ' '.join(lemms)\n",
    "    return doc\n",
    "\n",
    "# Fonction de préparation d'un doc passant par un stemming\n",
    "def transform_bow_stem_fct(doc) :\n",
    "    stems = stemming(doc)  \n",
    "    doc = ' '.join(stems)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.8 s, sys: 75.5 ms, total: 5.88 s\n",
      "Wall time: 5.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Prétraitement textuels\n",
    "\n",
    "# tokenisation\n",
    "df['TOKEN'] = df['TEXT'].map(tokenize)\n",
    "\n",
    "# stemming\n",
    "df['STEM'] = df['TEXT'].map(stemming)\n",
    "\n",
    "# Lemmatisation\n",
    "df['LEMM'] = df['TEXT'].map(lemmatisation)\n",
    "\n",
    "# tokenisation + transformation en phrases\n",
    "df['sentence_bow'] = df['TEXT'].map(transform_bow_fct)\n",
    "\n",
    "# stemming + transformation en phrases\n",
    "df['sentence_stem'] = df['TEXT'].map(transform_bow_stem_fct)\n",
    "\n",
    "# Lemmatisation + transformation en phrases\n",
    "df['sentence_lemm'] = df['TEXT'].map(transform_bow_lemm_fct)\n",
    "\n",
    "# tokenisation pour modèle DL + transformation en phrases\n",
    "df['sentence_dl'] = df['TEXT'].map(transform_dl_fct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de mots maximum dans le bag-of-words avec la méthode de tokénisation : 330\n",
      "Nombre de mots maximum dans le bag-of-words avec la méthode de lemmatisation : 330\n",
      "Nombre de mots maximum dans le bag-of-words avec la méthode de stemming : 330\n",
      "Nombre de mots maximum pour le deep learning (DL) avec la méthode de tokénisation : 562\n"
     ]
    }
   ],
   "source": [
    "# Taille maximum des features selon les techniques de prétraitement du texte\n",
    "df['length_bow'] = df['sentence_bow'].apply(lambda x : len(word_tokenize(x)))\n",
    "df['length_dl'] = df['sentence_dl'].apply(lambda x : len(word_tokenize(x)))\n",
    "df['length_bow_lemm'] = df['sentence_lemm'].apply(lambda x : len(word_tokenize(x)))\n",
    "df['length_bow_stem'] = df['sentence_stem'].apply(lambda x : len(word_tokenize(x)))\n",
    "\n",
    "print(\"Nombre de mots maximum dans le bag-of-words avec la méthode de tokénisation :\", df['length_bow'].max())\n",
    "print(\"Nombre de mots maximum dans le bag-of-words avec la méthode de lemmatisation :\", df['length_bow_lemm'].max())\n",
    "print(\"Nombre de mots maximum dans le bag-of-words avec la méthode de stemming :\", df['length_bow_stem'].max())\n",
    "print(\"Nombre de mots maximum pour le deep learning (DL) avec la méthode de tokénisation :\", df['length_dl'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Type de modification</th>\n",
       "      <th>Test on sample 220</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0. TEXT</th>\n",
       "      <td>Poppins Printed Baby Boy's Jumpsuit\\r\\n       ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1. CLEAN_TEXT</th>\n",
       "      <td>poppins printed baby boy jumpsuit high quality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2. TOKENIZATION</th>\n",
       "      <td>['poppins', 'printed', 'baby', 'boy', 'jumpsui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2B. TOKENIZED DOC</th>\n",
       "      <td>poppins printed baby boy jumpsuit high quality...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3. STEMMING</th>\n",
       "      <td>['poppin', 'print', 'babi', 'boy', 'jumpsuit',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3B. STEMMATIZED DOC</th>\n",
       "      <td>poppin print babi boy jumpsuit high qualiti fu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4. LEMMATIZATION</th>\n",
       "      <td>['poppins', 'printed', 'baby', 'boy', 'jumpsui...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4B. LEMMATIZED DOC</th>\n",
       "      <td>poppins printed baby boy jumpsuit high quality...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Type de modification                                 Test on sample 220\n",
       "0. TEXT               Poppins Printed Baby Boy's Jumpsuit\\r\\n       ...\n",
       "1. CLEAN_TEXT         poppins printed baby boy jumpsuit high quality...\n",
       "2. TOKENIZATION       ['poppins', 'printed', 'baby', 'boy', 'jumpsui...\n",
       "2B. TOKENIZED DOC     poppins printed baby boy jumpsuit high quality...\n",
       "3. STEMMING           ['poppin', 'print', 'babi', 'boy', 'jumpsuit',...\n",
       "3B. STEMMATIZED DOC   poppin print babi boy jumpsuit high qualiti fu...\n",
       "4. LEMMATIZATION      ['poppins', 'printed', 'baby', 'boy', 'jumpsui...\n",
       "4B. LEMMATIZED DOC    poppins printed baby boy jumpsuit high quality..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Premier exemple pour chaque type de modification (CE6)\n",
    "row_index = 220\n",
    "example_df = pd.DataFrame({'0. TEXT': df['TEXT'][row_index], \n",
    "                           '1. CLEAN_TEXT': cleaning_text(df['TEXT'][row_index]),\n",
    "                           '2. TOKENIZATION': str(tokenize(df['TEXT'][row_index])),\n",
    "                           '2B. TOKENIZED DOC': str(transform_bow_fct(df['TEXT'][row_index])),\n",
    "                           '3. STEMMING': str(stemming(df['TEXT'][row_index])),\n",
    "                           '3B. STEMMATIZED DOC': str(transform_bow_stem_fct(df['TEXT'][row_index])),\n",
    "                           '4. LEMMATIZATION': str(lemmatisation(df['TEXT'][row_index])),\n",
    "                           '4B. LEMMATIZED DOC': str(transform_bow_lemm_fct(df['TEXT'][row_index])),\n",
    "                          }, index=['Test on sample 220']).rename_axis('Type de modification').T\n",
    "example_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocds6",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
